---
title: "Project 3 - COVID19 Model"
author: "Margaret Gacheru, Joy Hsu, Melanie Mayer, Rachel Tsong, Adina Zhang"
date: "4/16/2020"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
require(drc)
```

Read in data:

```{r data}
covid19 <- read_csv("covid19-1.csv")
```

Explore data - NY Example:

```{r explore}
NY_dat = covid19 %>% filter(`Province/State` == "New York")

#Initialize column - time from 1st case
NY_dat$time_from_first_case <- rep(0, dim(NY_dat)[1])
#Find location of first confirmed case
case = sum(NY_dat$ConfirmedCases == 0)
#Fill in column
j = 1
for (i in (case+1):dim(NY_dat)[1]) {
  NY_dat$time_from_first_case[i] = j
  j = j+1
}

#Check whether logistic curve is good aproximation - cases
plot(ConfirmedCases ~ time_from_first_case, data = NY_dat) 

#Initialize column - time from 1st death
NY_dat$time_from_first_death <- rep(0, dim(NY_dat)[1])
#Find location of first confirmed case
deaths = sum(NY_dat$Fatalities == 0)
#Fill in column
j = 1
for (i in (deaths+1):dim(NY_dat)[1]) {
  NY_dat$time_from_first_death[i] = j
  j = j+1
}

#Check whether logistic curve is good aproximation - fatalities
plot(Fatalities ~ time_from_first_death, data = NY_dat) 

#Appears to have exponential growth, will try logistic growth model however
```

Estimation of logistic growth curve parameters using R functions:

```{r curve_params, eval = F}
model <- drc::drm(ConfirmedCases ~ time_from_first_case, fct = L.3(), data = NY_dat)
plot(model, log="", main = "Logistic function")

model <- drc::drm(Fatalities ~ time_from_first_death, fct = L.3(), data = NY_dat)
plot(model, log="", main = "Logistic function")

```

Create time from first case/fatality variables for entire dataset by Country/Region:

```{r data_cleaning}
#create desired variables, remove regions with less than 14 days with cases
covid19_country <- covid19 %>%
  group_by(`Country/Region`, Date) %>%
  summarise(ConfirmedCases = sum(ConfirmedCases),
            Fatalities = sum(Fatalities)) %>% 
  filter(ConfirmedCases != 0) %>%
  mutate(Date = as.Date(Date, format="%m/%d/%Y")) %>%
  arrange(`Country/Region`, Date) %>%
  mutate(
    time_from_first_case = ifelse(ConfirmedCases >= 1, 1, 0),
         time_from_first_case = cumsum(time_from_first_case),
         time_from_first_death = ifelse(Fatalities >= 1, 1, 0),
         time_from_first_death = cumsum(time_from_first_death)) %>%
  filter(max(time_from_first_case) >= 14)
```

Estimate Logistic Curve by Country/Region:

```{r estimates}
#Find estimates per country of cases - premade R function
cases_country_curves <- covid19_country %>%
  split(.$`Country/Region`) %>%
  map(~coefficients(drm(ConfirmedCases ~ time_from_first_case, fct = L.3(), data = .)))

cases_country_curves <- data.frame(matrix(unlist(cases_country_curves), nrow=length(cases_country_curves), byrow=T))

cases_country_curves <- cbind(cases_country_curves, Country = levels(factor(covid19_country$`Country/Region`))) %>%
  rename(B = "X1", A = "X2", C = "X3")


#Find estimates per country of fatalities - premade R function
#Fails to converge, use oy countries with time since first death >= 14
covid19_country_fatal <- covid19_country %>%
  filter(max(time_from_first_death) >= 14)
fatalities_country_curves <- covid19_country_fatal %>% 
  split(.$`Country/Region`) %>%
  map(~coefficients(drm(Fatalities ~ time_from_first_death, fct = L.3(), data = .)))

fatalities_country_curves <- data.frame(matrix(unlist(fatalities_country_curves), nrow=length(fatalities_country_curves), byrow=T))

fatalities_country_curves <- cbind(fatalities_country_curves, Country = levels(factor(covid19_country_fatal$`Country/Region`))) %>%
  rename(B = "X1", A = "X2", C = "X3")
```

## Task 2. Clustering your fitted Curves

#### K-mean Clustering:

- K clusters: $C_{1}, C_{2}, ...., C_{k}$
- Want to minimize within cluster correlation: minimize $$\sum_{k=1}^{K} W(C_{k})$$
- Define within cluster variance using the squared Euclidian distance:  $$W(C_{k}) = \frac{1}{|x|}\sum_{i,i' \in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j})^2$$ where $|C_{k}|$ is the number of observations in cluster k

Algorithm:

1. Randomly choose k observations, use chosen observations' p observed values as centroid
2. Assign each observation to the cluster whose centroid is closest based on sum of p Euclidian distances
3. Compute p * k cluster means
3. Iterate step 2 & 3 until stop changing

```{r kmean}
kmeans_cluster <- function(X, k){
  #X: data frame
  #k: number of clusters desired
  p <- dim(X)[2]  # number of parameters
  n <- dim(X)[1]  # number of observations
  delta <- 1
  iter <- 0 
  itermax <- 30
  while(delta > 1e-4 && iter <= itermax){
    if(iter == 0){
      centroid <- X[sample(n, k),] #Initiate, randomly pick three observations, use values as centroid
      centroid_mem <- centroid
    }
    
    d <- sapply(1:k, function(c) sapply(1:n, 
      function(i) sum((centroid[c,] - X[i,])^2))) #sum of p Euclidian distances from k centroids per obs 
    
    cluster <- apply(d, 1, which.min) #Place obs. in cluster with smallest distance
    
    centroid <- t(sapply(1:k, function(c) 
      apply(X[cluster == c,], 2, mean))) # Compute new k*p centroids
    
    delta <- sum((centroid - centroid_mem)^2) #Check converegence
    iter <- iter + 1 
    centroid_mem <- centroid
  }
  X = cbind(X, cluster = cluster)
  return(list(centroid = centroid, cluster = cluster, df = X))
}

# run K-means
km <- kmeans_cluster(cases_country_curves[,1:3], 2)
pairs(cases_country_curves[,1:3], lower.panel = NULL, col = km$cluster)
summary(factor(km$cluster))

#Compare to package
km_pkg <- kmeans(cases_country_curves[,1:3], 2)
summary(factor(km_pkg$cluster))
km_pkg_vec <- cbind(cases_country_curves, cluster = km_pkg$cluster)

```


The Gaussian Mixture Model was applied using EM algorithm to cluster the fitted parameters. The EM algorithm allows for maximizing the likelihood function when some of the variables are unobserved. In this case unobserved variable would refer to the clusters. 

In the algorithm, the first step is the Expectation step in which the probability of being in a cluter given the current data is calculated. 


```{r}
# EM algorithm 
# distribution to estimate cluster probability
mvnorm_covinv = function(Sigma) {
  # Eigendecomposition of covariance matrix
  E = eigen(Sigma)
  Lambda_inv = diag(E$values^-1)   # diagonal matrix
  Q = E$vectors
  return(Q %*% Lambda_inv %*% t(Q))
}
# multivariate Gaussian pdf 
mvn_pdfi = function(xi, mu, Sigma)
  1/sqrt( (2*pi)^length(xi) * det(Sigma) ) * 
  exp(-(1/2) * t(xi - mu) %*% mvnorm_covinv(Sigma) 
  %*% (xi - mu))
# for all X
mvn_pdf = function(X, mu, Sigma)
  apply(X, 1, function(xi) mvn_pdfi(as.numeric(xi), mu, Sigma))

# function for clustering
gmm_clustering = function(X, k){
  p = ncol(X)  # number of parameters
  n = nrow(X)  # number of observations
  Delta = 1
  iter = 0
  itermax = 30
  while(Delta > 1e-4 && iter <= itermax){
    # initiation
    if(iter == 0){
      # use centroid from kmeans algorithm 
      km_init = kmeans_cluster(X, k)
      mu = km_init$centroid
      mu_mem = mu
      w = sapply(1:k, function(i) length(which(km_init$cluster == i)))
      # assign starting weights based on K means results
      w = w/sum(w)
      cov = array(dim = c(p, p, k))
      for(i in 1:p) for(j in 1:p) for(c in 1:k)
        # starting sigma matrices for mvn distribution 
        cov[i, j, c] = 
        1/n * sum((X[km_init$cluster == c, i] - mu[c, i]) *
        (X[km_init$cluster == c, j] - mu[c, j]))
    }
    
 # E-step
    mvn_c = sapply(1:k, function(c) mvn_pdf(X, mu[c,], cov[,, c]))
    # probability of point in cluster c 
    p_ic = t(w*t(mvn_c)) / rowSums(t(w*t(mvn_c)))
    
 # M-step
    n_c = colSums(p_ic)
    w = n_c/sum(n_c)
    mu = t(sapply(1:k, function(c) 1/n_c[c] * colSums(p_ic[, c] *
      X)))
    for(i in 1:p) for(j in 1:p) for(c in 1:k) 
      cov[i, j, c] =
      1/n_c[c] * sum(p_ic[, c] * (X[, i] - mu[c, i]) * p_ic[, c] *
      (X[, j] - mu[c, j]))
    Delta = sum((mu - mu_mem)^2)
    iter = iter + 1
    mu_mem = mu
  }
  return(list(softcluster = p_ic, cluster = apply(p_ic, 1,
    which.max)))
}

# run for two clusters
gmm = gmm.fromscratch(cases_country_curves[,1:3], 2)
```

