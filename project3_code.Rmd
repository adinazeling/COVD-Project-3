---
title: "Project 3 - COVID19 Model"
author: "Margaret Gacheru, Joy Hsu, Melanie Mayer, Rachel Tsong, Adina Zhang"
date: "4/16/2020"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
require(drc)
```

Read in data:

```{r data}
covid19 <- read_csv("covid19-1.csv")
```

Explore data - NY Example:

```{r explore}
NY_dat = covid19 %>% filter(`Province/State` == "New York")

#Initialize column - time from 1st case
NY_dat$time_from_first_case <- rep(0, dim(NY_dat)[1])
#Find location of first confirmed case
case = sum(NY_dat$ConfirmedCases == 0)
#Fill in column
j = 1
for (i in (case+1):dim(NY_dat)[1]) {
  NY_dat$time_from_first_case[i] = j
  j = j+1
}

#Check whether logistic curve is good aproximation - cases
plot(ConfirmedCases ~ time_from_first_case, data = NY_dat) 

#Initialize column - time from 1st death
NY_dat$time_from_first_death <- rep(0, dim(NY_dat)[1])
#Find location of first confirmed case
deaths = sum(NY_dat$Fatalities == 0)
#Fill in column
j = 1
for (i in (deaths+1):dim(NY_dat)[1]) {
  NY_dat$time_from_first_death[i] = j
  j = j+1
}

#Check whether logistic curve is good aproximation - fatalities
plot(Fatalities ~ time_from_first_death, data = NY_dat) 

#Appears to have exponential growth, will try logistic growth model however
```

Estimation of logistic growth curve parameters using R functions:

```{r curve_params, eval = F}
model <- drc::drm(ConfirmedCases ~ time_from_first_case, fct = L.3(), data = NY_dat)
plot(model, log="", main = "Logistic function")

model <- drc::drm(Fatalities ~ time_from_first_death, fct = L.3(), data = NY_dat)
plot(model, log="", main = "Logistic function")

```

<<<<<<< HEAD

Newton Raphson

```{r,eval=F}

NewtonRaphson = function(data, main_function, start, tol = 1e-10, maxiter = 200) {
  #convert data into matrix form
  model_data = data%>%
    select(-c("diagnosis"))
  intercept = rep(1, dim(model_data)[1])
  #X = as.matrix(cbind(intercept, scale(model_data, scale = TRUE)))
  X = as.matrix(cbind(intercept, model_data))
  Y = as.matrix(as.integer(data$diagnosis == "M"))
  Beta = as.matrix(start, nrow = dim(model_data)[2] + 1)
    
  i = 0
  cur = Beta
  lik_grad_hess = main_function(Y, X, cur)
  res = c(0, lik_grad_hess$loglik, cur)
  step = 1
  
  prevloglik = -Inf # To make sure it iterates
  
  diff_loglik = abs(lik_grad_hess$loglik - prevloglik)
  #if (is.nan(diff_loglik)) { diff_loglik <- 1e-2 }
  
  while(i < maxiter && diff_loglik > tol) {
    i = i + 1
    
    prevlik_grad_hess = lik_grad_hess #time step i - 1
    prevloglik = prevlik_grad_hess$loglik
    prev = cur #step i - 1

    
    #ensure that the direction of the step is in ascent direction
    d_grad = - t(prevlik_grad_hess$grad) %*% ginv(prevlik_grad_hess$Hess, 2.9876e-18 ) %*% (prevlik_grad_hess$grad)
    
    #max_eig = max(eigen(prevlik_grad_hess$Hess)$values)
    n = ncol(prevlik_grad_hess$Hess)
    gamma = 0.01
      
    while (d_grad <= 0){
      
      prevlik_grad_hess$Hess = prevlik_grad_hess$Hess - gamma*diag(n)
      
      d_grad = - t(prevlik_grad_hess$grad) %*% ginv(prevlik_grad_hess$Hess, 2.9876e-18 ) %*% (prevlik_grad_hess$grad)
      
      gamma = gamma + 0.01
    }
    
    cur = prev - ginv(prevlik_grad_hess$Hess, 2.9876e-18) %*% prevlik_grad_hess$grad #step find theta for step i 
    lik_grad_hess = main_function(Y, X, cur) #update log-lik, gradient, Hessian for step i 

    while (lik_grad_hess$loglik < prevloglik) {
      
      step = 0.5*step
      cur = prev - step * ginv(prevlik_grad_hess$Hess, 2.9876e-18) %*% prevlik_grad_hess$grad
      lik_grad_hess = main_function(Y, X, cur)
      
      }
    
    res = rbind(res, c(i, lik_grad_hess$loglik, cur)) 
    
    diff_loglik = abs(lik_grad_hess$loglik - prevloglik)
    if (is.nan(diff_loglik)) { diff_loglik <- 1e-2 }
    
    }
  
  return(res)
  
  }

path = NewtonRaphson(breastcancer_data, main_function, start = rep(0, dim(data)[2]))
path[dim(path)[1],]

```



=======
>>>>>>> d8957a4384738f8ad02e2d5c8e8d50dc523bc8b9
Create time from first case/fatality variables for entire dataset by Country/Region:

```{r data_cleaning}
#create desired variables, remove regions with less than 14 days with cases
covid19_country <- covid19 %>%
  group_by(`Country/Region`, Date) %>%
  summarise(ConfirmedCases = sum(ConfirmedCases),
            Fatalities = sum(Fatalities)) %>% 
  filter(ConfirmedCases != 0) %>%
  mutate(Date = as.Date(Date, format="%m/%d/%Y")) %>%
  arrange(`Country/Region`, Date) %>%
  mutate(
    time_from_first_case = ifelse(ConfirmedCases >= 1, 1, 0),
         time_from_first_case = cumsum(time_from_first_case),
         time_from_first_death = ifelse(Fatalities >= 1, 1, 0),
         time_from_first_death = cumsum(time_from_first_death)) %>%
  filter(max(time_from_first_case) >= 14)
```

Estimate Logistic Curve by Country/Region:

```{r estimates}
#Find estimates per country of cases - premade R function
cases_country_curves <- covid19_country %>%
  split(.$`Country/Region`) %>%
  map(~coefficients(drm(ConfirmedCases ~ time_from_first_case, fct = L.3(), data = .)))

cases_country_curves <- data.frame(matrix(unlist(cases_country_curves), nrow=length(cases_country_curves), byrow=T))

cases_country_curves <- cbind(cases_country_curves, Country = levels(factor(covid19_country$`Country/Region`))) %>%
  rename(B = "X1", A = "X2", C = "X3")


#Find estimates per country of fatalities - premade R function
#Fails to converge, use oy countries with time since first death >= 14
covid19_country_fatal <- covid19_country %>%
  filter(max(time_from_first_death) >= 14)
fatalities_country_curves <- covid19_country_fatal %>% 
  split(.$`Country/Region`) %>%
  map(~coefficients(drm(Fatalities ~ time_from_first_death, fct = L.3(), data = .)))

fatalities_country_curves <- data.frame(matrix(unlist(fatalities_country_curves), nrow=length(fatalities_country_curves), byrow=T))

fatalities_country_curves <- cbind(fatalities_country_curves, Country = levels(factor(covid19_country_fatal$`Country/Region`))) %>%
  rename(B = "X1", A = "X2", C = "X3")
```

## Task 2. Clustering your fitted Curves

#### K-mean Clustering:

- K clusters: $C_{1}, C_{2}, ...., C_{k}$
- Want to minimize within cluster correlation: minimize $$\sum_{k=1}^{K} W(C_{k})$$
- Define within cluster variance using the squared Euclidian distance:  $$W(C_{k}) = \frac{1}{|x|}\sum_{i,i' \in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j})^2$$ where $|C_{k}|$ is the number of observations in cluster k

Algorithm:

1. Randomly choose k observations, use chosen observations' p observed values as centroid
2. Assign each observation to the cluster whose centroid is closest based on sum of p Euclidian distances
3. Compute p * k cluster means
3. Iterate step 2 & 3 until stop changing

```{r kmean}
kmeans_cluster <- function(X, k){
  #X: data frame
  #k: number of clusters desired
  p <- dim(X)[2]  # number of parameters
  n <- dim(X)[1]  # number of observations
  delta <- 1
  iter <- 0 
  itermax <- 30
  
  centroid <- X[sample(n, k),] #Initiate, randomly pick three observations, use values as centroid
  centroid_mem <- centroid
  
  while(delta > 1e-4 && iter <= itermax){
    d <- sapply(1:k, function(c) sapply(1:n, 
      function(i) sum((centroid[c,] - X[i,])^2))) #sum of p Euclidian distances from k centroids per obs 
    
    cluster <- apply(d, 1, which.min) #Place obs. in cluster with smallest distance
    
    centroid <- t(sapply(1:k, function(c) 
      apply(X[cluster == c,], 2, mean))) # Compute new k*p centroids
    
    delta <- sum((centroid - centroid_mem)^2) #Check converegence
    iter <- iter + 1 
    centroid_mem <- centroid
  }
  X = cbind(X, cluster = cluster)
  return(list(centroid = centroid, cluster = cluster, df = X))
}

# run K-means
km <- kmeans_cluster(cases_country_curves[,1:3], 2)
pairs(cases_country_curves[,1:3], lower.panel = NULL, col = km$cluster)
summary(factor(km$cluster))

#Compare to package
km_pkg <- kmeans(cases_country_curves[,1:3], 2)
summary(factor(km_pkg$cluster))
km_pkg_vec <- cbind(cases_country_curves, cluster = km_pkg$cluster)

```


The Gaussian Mixture Model (GMM) was applied using EM algorithm to cluster the fitted parameters. The EM algorithm allows for maximizing the likelihood function when some of the variables are unobserved. In this case unobserved variable would refer to the clusters.Since this is a GMM, the parameters are assumed to follow a multivariate normal distribution with mean $\mu$ and covariance matrix $\sum$. 

In the algorithm, the first step is the Expectation step in which the probability of being in a cluter given the current data is calculated. The expectation can be represented as follows:
$$E[Z_i=1|x_i, \theta^{(t)}]=P(Z_i=1|x_i, \theta^{(t)}) = \frac{p^{(t)}f(x_i, \mu^{(t)}_2, \sum^{(t)}_2)}{(1-p^{(t)})f(x_i, \mu^{(t)}_1, \sum^{(t)}_1)+p^{(t)}f(x_i, \mu^{(t)}_2, \sum^{(t)}_2)}$$
with $Z_i$ indicating the cluster. So if $Z_i=1$ then $X_i$ would be from the $MVN(\mu_2, \sum_2)$ distribution. For the initiation, the results of the K-means clustering was used as the starting values for the weights, means, and covariance matrices.


The second step is the Maximizing step wherein the likelihood function is maximized to update the parameters. More specificaly, the cluster probabilities (i.e. the weight signifiying how much each cluster represents the data points), cluster means, and cluster covariance matrices will be updated. The equations for the parameters are as follows:


These two steps are repeated iteratively until the parameters converge (change less than 0.00001) or the max number of iterations is reached. 


```{r}
# EM algorithm 
# distribution to estimate cluster probability
mvnorm_covinv = function(Sigma) {
  # Eigendecomposition of covariance matrix
  E = eigen(Sigma)
  Lambda_inv = diag(E$values^-1)   # diagonal matrix
  Q = E$vectors
  return(Q %*% Lambda_inv %*% t(Q))
}
# multivariate Gaussian pdf 
mvn_pdfi = function(xi, mu, Sigma)
  1/sqrt( (2*pi)^length(xi) * det(Sigma) ) * 
  exp(-(1/2) * t(xi - mu) %*% mvnorm_covinv(Sigma) 
  %*% (xi - mu))
# for all X
mvn_pdf = function(X, mu, Sigma)
  apply(X, 1, function(xi) mvn_pdfi(as.numeric(xi), mu, Sigma))

# function for clustering
gmm_clustering = function(X, k){
  p = ncol(X)  # number of parameters
  n = nrow(X)  # number of observations
  Delta = 1
  iter = 0
  itermax = 30
  while(Delta > 1e-4 && iter <= itermax){
    # initiation
    if(iter == 0){
      # use centroid from kmeans algorithm 
      km_init = kmeans_cluster(X, k)
      mu = km_init$centroid
      mu_mem = mu
      w = sapply(1:k, function(i) length(which(km_init$cluster == i)))
      # assign starting weights based on K means results
      w = w/sum(w)
      cov = array(dim = c(p, p, k))
      for(i in 1:p) for(j in 1:p) for(c in 1:k)
        # starting sigma matrices for mvn distribution 
        cov[i, j, c] = 
        1/n * sum((X[km_init$cluster == c, i] - mu[c, i]) *
        (X[km_init$cluster == c, j] - mu[c, j]))
    }
    
 # E-step
    mvn_c = sapply(1:k, function(c) mvn_pdf(X, mu[c,], cov[,, c]))
    # probability of point in cluster c 
    p_ic = t(w*t(mvn_c)) / rowSums(t(w*t(mvn_c)))
    
 # M-step
    n_c = colSums(p_ic)
    w = n_c/sum(n_c)
    mu = t(sapply(1:k, function(c) 1/n_c[c] * colSums(p_ic[, c] *
      X)))
    for(i in 1:p) for(j in 1:p) for(c in 1:k) 
      cov[i, j, c] =
      1/n_c[c] * sum(p_ic[, c] * (X[, i] - mu[c, i]) * p_ic[, c] *
      (X[, j] - mu[c, j]))
    Delta = sum((mu - mu_mem)^2)
    iter = iter + 1
    mu_mem = mu
  }
  return(list(softcluster = p_ic, cluster = apply(p_ic, 1,
    which.max)))
}

# run for two clusters
gmm = gmm_clustering(cases_country_curves[,1:3], 2)
```

