---
title: "newton_raphson_mg"
author: "Margaret Gacheru - mg3861"
date: "4/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

# Write a function that generates loss, gradient and Hessian
# Inputs: 
# t - days since first case
# y - outcome
# par - vector containing a, b, and c parameters
func = function(t, y, par) {
  
  a = par[1]
  b = par[2]
  c = par[3]
  
  # Expu
  expu = exp(-b * (t - c))
  
  # Loss function
  loss = -(1/2) * sum(y - (a / (1 + expu)))^2
  
  # First derivative matrix
  d1loss = vector(mode = "list")
  d1loss[[1]] = (1 / (1 + expu))
  d1loss[[2]] = (a * (c - t) * expu) / (1 + expu)^2
  d1loss[[3]] = (a * b * expu) / (1 + expu)^2
  
  # Gradient
  grad = vector(mode = "numeric", length = 3)
  
  grad[[1]] = -sum(y - (a / (1 + expu)) * d1loss[[1]])
  grad[[2]] = sum(y - (a / (1 + expu)) * d1loss[[2]])
  grad[[3]] = sum(y - (a / (1 + expu)) * d1loss[[3]])
  
  # Second derivative matrix
  d2loss = matrix(0, 3, 3)
  d2loss[1,1] = sum( ( 1/( 1+expu) )^2 )
  d2loss[1,2] = sum( (y*(c-t)*expu) / (1+expu)^2 ) + sum( (2*a*(c-t)*expu) / (1+expu)^3 )
  d2loss[1,3] = sum( (y*b*expu) / (1+expu)^2 ) + sum( (2*a*b*expu) / (1+expu)^3 )
  d2loss[2,2] = sum( (a*y* exp(b*c + b*t) * (exp(t*b) - exp(c*b)) * (c-t)^2) / (exp(c*b) + exp(t*b))^3 ) + sum( (a*a*expu*(2*expu - 1)*(c-t)^2) / (1+expu)^4 )
  d2loss[2,3] = sum( (a*y*expu*(expu*(b*c - t*b - 1) - c*b + t*b - 1)) / (1+expu)^3 ) + sum( (a*a*expu*(expu*(2*b*c - 2*t*b - 1) - c*b + t*b - 1)) / (1+expu)^4 )
  d2loss[3,3] = sum( (a*b*b*y*exp(b*t + b*c)*(exp(b*t) - exp(b*c))) / (exp(b*t) + exp(b*c))^3 ) + sum( (a*a*b*b*expu*(2*expu -1)) / (1+expu)^4 )
  d2loss[2,1] = d2loss[1,2]
  d2loss[3,1] = d2loss[1,3]
  d2loss[3,2] = d2loss[2,3]  
  
  hess = diag(3)
  
  return(list(loglik = loss, grad = grad, Hess = hess)) 
}


```

Newton Raphson

```{r}
# Half-Stepping and ascent gradients
NR_modify = function(t, y, start, tol = 1e-10, maxiter = 200){
  
  # Initiate starting values
  i = 0
  cur = start
  stuff = func(t, y, cur)
  res = c(0, stuff$loss, cur)
  currloss = stuff$loss
  prevloss = -Inf
  
  # Iterations to update the losslihood
  while (i < maxiter && abs(currloss - prevloss) > tol) {
    # Update step
    i = i + 1
    step = 1
    print(paste("step:", i, "  loss:", stuff$loss))
    
    # Assign previous log likelihoods and betas
    prevloss = currloss
    prev = cur
    
    # Next step of log likelihood
    cur = prev - step * solve(stuff$Hess) %*% stuff$grad
    
    # New losslihood, gradient, and Hessian 
    stuff = func(t, y, cur)
    currloss = stuff$loss
    
    # Ascent direction check
    ascent_dir = -t(stuff$grad) %*% solve(stuff$Hess) %*% stuff$grad
    if (ascent_dir < 0) {
      
      max_eigen = max(eigen(stuff$Hess)$values)
      Hess_new = stuff$Hess - (max_eigen + 0.1)
      
      } else {Hess_new = stuff$Hess}
    
    # Half-step check
    if (currloss > prevloss) {
      res = rbind(res, c(i, stuff$loss, cur))
    }
    else {
      while (currloss < prevloss) {
        step = step * (1/2)
        cur = prev - step * solve(Hess_new) %*% stuff$grad
        stuff = func(t, y, cur)
        currloss = stuff$loss
        }
    
      res = rbind(res, c(i, stuff$loss, cur))
    } 
    
  }
  return(res)
}
```

Version 2 of Newton Raphson

```{r}

NewtonRaphson = function(y, t, main_function, start, tol = 1e-10, maxiter = 200) {
 
  i = 0
  cur = start
  lik_grad_hess = main_function(t, y, cur)
  res = c(0, lik_grad_hess$loglik, cur)
  step = 1

  prevloglik = -Inf # To make sure it iterates
  
  diff_loglik = abs(lik_grad_hess$loglik - prevloglik)
  #if (is.nan(diff_loglik)) { diff_loglik <- 1e-2 }
  
  while(i < maxiter && diff_loglik > tol) {
    i = i + 1
    
    prevlik_grad_hess = lik_grad_hess #time step i - 1
    prevloglik = prevlik_grad_hess$loglik
    prev = cur #step i - 1

    
    #ensure that the direction of the step is in ascent direction
    d_grad = - t(prevlik_grad_hess$grad) %*% ginv(prevlik_grad_hess$Hess, 2.9876e-18 ) %*% (prevlik_grad_hess$grad)
    
    #max_eig = max(eigen(prevlik_grad_hess$Hess)$values)
    n = ncol(prevlik_grad_hess$Hess)
    gamma = 0.01
     
    #while (d_grad <= 0){
      
     # prevlik_grad_hess$Hess = prevlik_grad_hess$Hess - gamma*diag(n)
      
      #d_grad = - t(prevlik_grad_hess$grad) %*% ginv(prevlik_grad_hess$Hess, 2.9876e-18 ) %*% (prevlik_grad_hess$grad)
      
      #gamma = gamma + 0.01
    #}
    
    cur = prev - ginv(prevlik_grad_hess$Hess, 2.9876e-18) %*% prevlik_grad_hess$grad #step find theta for step i 
    lik_grad_hess = main_function(t, y, cur) #update log-lik, gradient, Hessian for step i 

    while (lik_grad_hess$loglik < prevloglik) {
      
      step = 0.5*step
      cur = prev - step * ginv(prevlik_grad_hess$Hess, 2.9876e-18) %*% prevlik_grad_hess$grad
      lik_grad_hess = main_function(t, y, cur)

      }
    
    res = rbind(res, c(i, lik_grad_hess$loglik, cur)) 
    
    diff_loglik = abs(lik_grad_hess$loglik - prevloglik)
    if (is.nan(diff_loglik)) { diff_loglik <- 1e-2 }
    
    }
  
  return(res)
  
  }
```


Test out func()

```{r}

check = func(NY_dat$time_from_first_case, NY_dat$ConfirmedCases, c(10, 10, 10))

test = NR_modify(NY_dat$time_from_first_case, NY_dat$ConfirmedCases, c(10, 10, 10))
NewtonRaphson(NY_dat$ConfirmedCases, NY_dat$time_from_first_case, func, c(1,1, 1))

```

